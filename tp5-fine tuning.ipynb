{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP5: Pre-training and Fine-Tuning of a Language Model\n",
    "\n",
    "#### Part 2: Fine-Tuning with BERT\n",
    "- Use a pre-trained BERT model for a binary classification task (e.g., positive/negative reviews).\n",
    "\n",
    "#### 1. Dataset:\n",
    "- Provide a simple dataset or use the IMDB dataset (available in `torchtext` for review classification).\n",
    "- Example data:\n",
    "    - \"I love this movie!\" ‚Üí Positive (1)\n",
    "    - \"This movie is horrible.\" ‚Üí Negative (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.9/site-packages (2.5.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.9/site-packages (2.2.3)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.9/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, threadpoolctl, scipy, safetensors, regex, pyyaml, joblib, idna, charset-normalizer, certifi, scikit-learn, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 huggingface-hub-0.26.3 idna-3.10 joblib-1.4.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "! pip install torch pandas transformers scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  label\n",
      "0                                  I love this movie!      1\n",
      "1                             This movie is horrible.      0\n",
      "2                 An incredible cinematic experience.      1\n",
      "3                    I wouldn't recommend this movie.      0\n",
      "4                     The actors did a fantastic job.      1\n",
      "5                The plot was boring and predictable.      0\n",
      "6                               A modern masterpiece.      1\n",
      "7                     I fell asleep during the movie.      0\n",
      "8         This is the best movie I've seen this year.      1\n",
      "9                           A complete waste of time.      0\n",
      "10               The cinematography was breathtaking.      1\n",
      "11                 The dialogues were poorly written.      0\n",
      "12                         A must-watch for everyone.      1\n",
      "13                   I regret spending money on this.      0\n",
      "14                The soundtrack added so much depth.      1\n",
      "15               The characters felt one-dimensional.      0\n",
      "16             A perfect blend of action and emotion.      1\n",
      "17            The ending was abrupt and unsatisfying.      0\n",
      "18           This movie exceeded all my expectations.      1\n",
      "19                    An absolute disaster of a film.      0\n",
      "20  The visuals were stunning but the story lacked...      1\n",
      "21            One of the worst movies I've ever seen.      0\n",
      "22              The humor was spot on and refreshing.      1\n",
      "23               Too slow-paced to keep my attention.      0\n",
      "24   A true classic that will stand the test of time.      1\n",
      "25            The performances were mediocre at best.      0\n",
      "26      I couldn't stop smiling throughout the movie.      1\n",
      "27              The plot twists were too predictable.      0\n",
      "28       An inspiring story with brilliant execution.      1\n",
      "29            Completely overhyped and disappointing.      0\n",
      "30                    Une ≈ìuvre d'art exceptionnelle.      1\n",
      "31              Je n'ai pas aim√© les effets sp√©ciaux.      0\n",
      "32              Les personnages sont tr√®s attachants.      1\n",
      "33          Le sc√©nario est confuse et mal structur√©.      0\n",
      "34           Une com√©die hilarante du d√©but √† la fin.      1\n",
      "35                    La bande sonore √©tait monotone.      0\n",
      "36               Un film √©mouvant qui touche le c≈ìur.      1\n",
      "37              Les dialogues manquent de profondeur.      0\n",
      "38          Une aventure palpitante et bien r√©alis√©e.      1\n",
      "39                  Le rythme du film est irr√©gulier.      0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"I love this movie!\",\n",
    "        \"This movie is horrible.\",\n",
    "        \"An incredible cinematic experience.\",\n",
    "        \"I wouldn't recommend this movie.\",\n",
    "        \"The actors did a fantastic job.\",\n",
    "        \"The plot was boring and predictable.\",\n",
    "        \"A modern masterpiece.\",\n",
    "        \"I fell asleep during the movie.\",\n",
    "        \"This is the best movie I've seen this year.\",\n",
    "        \"A complete waste of time.\",\n",
    "        \"The cinematography was breathtaking.\",\n",
    "        \"The dialogues were poorly written.\",\n",
    "        \"A must-watch for everyone.\",\n",
    "        \"I regret spending money on this.\",\n",
    "        \"The soundtrack added so much depth.\",\n",
    "        \"The characters felt one-dimensional.\",\n",
    "        \"A perfect blend of action and emotion.\",\n",
    "        \"The ending was abrupt and unsatisfying.\",\n",
    "        \"This movie exceeded all my expectations.\",\n",
    "        \"An absolute disaster of a film.\",\n",
    "        \"The visuals were stunning but the story lacked depth.\",\n",
    "        \"One of the worst movies I've ever seen.\",\n",
    "        \"The humor was spot on and refreshing.\",\n",
    "        \"Too slow-paced to keep my attention.\",\n",
    "        \"A true classic that will stand the test of time.\",\n",
    "        \"The performances were mediocre at best.\",\n",
    "        \"I couldn't stop smiling throughout the movie.\",\n",
    "        \"The plot twists were too predictable.\",\n",
    "        \"An inspiring story with brilliant execution.\",\n",
    "        \"Completely overhyped and disappointing.\",\n",
    "        \"Une ≈ìuvre d'art exceptionnelle.\",\n",
    "        \"Je n'ai pas aim√© les effets sp√©ciaux.\",\n",
    "        \"Les personnages sont tr√®s attachants.\",\n",
    "        \"Le sc√©nario est confuse et mal structur√©.\",\n",
    "        \"Une com√©die hilarante du d√©but √† la fin.\",\n",
    "        \"La bande sonore √©tait monotone.\",\n",
    "        \"Un film √©mouvant qui touche le c≈ìur.\",\n",
    "        \"Les dialogues manquent de profondeur.\",\n",
    "        \"Une aventure palpitante et bien r√©alis√©e.\",\n",
    "        \"Le rythme du film est irr√©gulier.\"\n",
    "    ],\n",
    "    'label': [\n",
    "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, \n",
    "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0, \n",
    "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1, 0, 1, 0, 1, 0\n",
    "    ] \n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the dataset\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare the Dataset\n",
    "- Split the data into training and validation sets using `train_test_split` from `sklearn`.\n",
    "- Tokenize using `BertTokenizer.from_pretrained` (from the `transformers` library)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 32 samples\n",
      "Validation set size: 8 samples\n",
      "Tokenization example for the first sample in the training set:\n",
      "tensor([   101,  10281, 106952,  10168,  10458,  10176,  10478,  14412,  45837,\n",
      "         11709,    119,    102,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the sizes of the datasets\n",
    "print(f\"Training set size: {len(train_df)} samples\")\n",
    "print(f\"Validation set size: {len(val_df)} samples\")\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize the training set texts\n",
    "train_encodings = tokenizer(\n",
    "    train_df['text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'  # Returns PyTorch tensors\n",
    ")\n",
    "\n",
    "# Tokenize the validation set texts\n",
    "val_encodings = tokenizer(\n",
    "    val_df['text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Display an example of tokenization\n",
    "print(\"Tokenization example for the first sample in the training set:\")\n",
    "print(train_encodings['input_ids'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load the Pre-trained Model:\n",
    "- Load a pre-trained BERT model ready for fine-tuning on a classification task.\n",
    "- `BertForSequenceClassification.from_pretrained` (from `transformers`):\n",
    "  To load BERT and add a dense layer for 2 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the BertForSequenceClassification class\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model for classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-multilingual-cased',  # Multilingual model suitable for French\n",
    "    num_labels=2  # Number of classes for binary classification (positive or negative)\n",
    ")\n",
    "\n",
    "# Display a summary of the model\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Adapt BERT for Classification:\n",
    "- Convert tokenized data into PyTorch-compatible datasets. \n",
    "- `TensorDataset` (from `torch.utils.data`): To transform encodings and labels into datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([8, 16])\n",
      "attention_mask shape: torch.Size([8, 16])\n",
      "labels: tensor([1, 1, 1, 1, 1, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Prepare labels for the training dataset\n",
    "train_labels = torch.tensor(train_df['label'].tolist())\n",
    "\n",
    "# Prepare labels for the validation dataset\n",
    "val_labels = torch.tensor(val_df['label'].tolist())\n",
    "\n",
    "# Create the TensorDataset for the training dataset\n",
    "train_dataset = TensorDataset(\n",
    "    train_encodings['input_ids'],\n",
    "    train_encodings['attention_mask'],\n",
    "    train_labels\n",
    ")\n",
    "\n",
    "# Create the TensorDataset for the validation dataset\n",
    "val_dataset = TensorDataset(\n",
    "    val_encodings['input_ids'],\n",
    "    val_encodings['attention_mask'],\n",
    "    val_labels\n",
    ")\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Get a training batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Extract elements from the batch\n",
    "input_ids, attention_mask, labels = batch\n",
    "\n",
    "print(\"input_ids shape:\", input_ids.shape)\n",
    "print(\"attention_mask shape:\", attention_mask.shape)\n",
    "print(\"labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Configure Training\n",
    "- Define training parameters (number of epochs, batch size, etc.).\n",
    "- Use `TrainingArguments` (from `transformers`) to define the training parameters.\n",
    "\n",
    "#### 6. Train the Model:\n",
    "- Use the training data to fine-tune the weights of BERT and the new classification layer.\n",
    "\n",
    "#### 7. Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/morgan/Desktop/EFREI/M2/LLM/TP5/.venv/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 4/12 [00:02<00:03,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.692333459854126, 'eval_accuracy': 0.5, 'eval_runtime': 0.0918, 'eval_samples_per_second': 87.149, 'eval_steps_per_second': 10.894, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 8/12 [00:09<00:03,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6924784183502197, 'eval_accuracy': 0.5, 'eval_runtime': 0.0731, 'eval_samples_per_second': 109.364, 'eval_steps_per_second': 13.671, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 10/12 [00:15<00:03,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7021, 'grad_norm': 6.165192604064941, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:19<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6930872797966003, 'eval_accuracy': 0.5, 'eval_runtime': 0.0756, 'eval_samples_per_second': 105.841, 'eval_steps_per_second': 13.23, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:24<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.1314, 'train_samples_per_second': 3.978, 'train_steps_per_second': 0.497, 'train_loss': 0.7021592855453491, 'epoch': 3.0}\n",
      "Evaluating the model on the validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 647.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00         4\n",
      "    Positive       0.50      1.00      0.67         4\n",
      "\n",
      "    accuracy                           0.50         8\n",
      "   macro avg       0.25      0.50      0.33         8\n",
      "weighted avg       0.25      0.50      0.33         8\n",
      "\n",
      "Text: 'This movie is a masterpiece!' - Prediction: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/morgan/Desktop/EFREI/M2/LLM/TP5/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/morgan/Desktop/EFREI/M2/LLM/TP5/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/morgan/Desktop/EFREI/M2/LLM/TP5/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Add a class to convert tuples to dictionaries\n",
    "class DictDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets compatible with the Trainer\n",
    "train_dataset = DictDataset(train_encodings, train_labels)\n",
    "val_dataset = DictDataset(val_encodings, val_labels)\n",
    "\n",
    "# 5. Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # Directory to save results\n",
    "    num_train_epochs=3,                 # Number of epochs\n",
    "    per_device_train_batch_size=8,      # Batch size for training\n",
    "    per_device_eval_batch_size=8,       # Batch size for evaluation\n",
    "    warmup_steps=500,                   # Number of warmup steps (scheduler)\n",
    "    weight_decay=0.01,                  # Weight decay (L2 regularization)\n",
    "    logging_dir='./logs',               # Directory for logs\n",
    "    logging_steps=10,                   # Logging frequency\n",
    "    evaluation_strategy=\"epoch\",        # Evaluation frequency (each epoch)\n",
    "    save_strategy=\"epoch\",              # Save at each epoch\n",
    "    load_best_model_at_end=True,        # Load the best model at the end\n",
    "    metric_for_best_model=\"accuracy\",   # Metric to determine the best model\n",
    "    save_total_limit=2                  # Save only the 2 best models\n",
    ")\n",
    "\n",
    "# Function to compute performance metrics\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Create the Trainer object to manage training and evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,                         # Model to train\n",
    "    args=training_args,                  # Training parameters\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Validation dataset\n",
    "    compute_metrics=compute_metrics      # Function to compute metrics\n",
    ")\n",
    "\n",
    "# 6. Train the model\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"./saved_model\")\n",
    "tokenizer.save_pretrained(\"./saved_model\")\n",
    "\n",
    "# 7. Test the model\n",
    "print(\"Evaluating the model on the validation set...\")\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# Compute and display performance metrics\n",
    "y_true = val_df['label'].tolist()\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n",
    "\n",
    "# Example: Test a new sentence\n",
    "test_sentence = \"This movie is a masterpiece!\"\n",
    "inputs = tokenizer(test_sentence, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {key: value.to(model.device) for key, value in inputs.items()}\n",
    "\n",
    "# Get the prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "predicted_class = np.argmax(logits.cpu().numpy(), axis=1)[0]\n",
    "print(f\"Text: '{test_sentence}' - Prediction: {'Positive' if predicted_class == 1 else 'Negative'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
