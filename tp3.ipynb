{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective of the Lab\n",
    "##### Manipulate a pre-trained language model to understand how it generates text and explore the process of tokenization and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating a Language Model with Hugging Face Transformers\n",
    "##### Required materials: Python, access to the Hugging Face `transformers` library (pre-installed if possible), access to Jupyter Notebook or a Python-compatible IDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Installation and Import of Libraries\n",
    "- Install the required libraries: `transformers`, `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.5.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.46.3\n",
      "Torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Import the Transformers library\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "# Checking PyTorch (torch)\n",
    "import torch\n",
    "\n",
    "# Display the versions of the installed libraries\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Torch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading a Pre-trained Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Tokenization of Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (as text): ['The', 'Ġartificial', 'Ġintelligence', 'Ġsystem']\n",
      "Tokens (as numeric indices): tensor([[  464, 11666,  4430,  1080]])\n"
     ]
    }
   ],
   "source": [
    "# Load the input text\n",
    "input_text = \"The artificial intelligence system\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = tokenizer.tokenize(input_text)\n",
    "token_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Tokens (as text):\", tokens)\n",
    "print(\"Tokens (as numeric indices):\", token_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Text Generation\n",
    "- Generate a continuation for this sentence.\n",
    "- Code to generate the text continuation: `model.generate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a text continuation\n",
    "output = model.generate(\n",
    "    token_ids,\n",
    "    max_length=50,  # Maximum length of the generated text\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    "    no_repeat_ngram_size=2,  # Prevent repetitions\n",
    "    top_k=50,  # Limit to a subset of probable words\n",
    "    top_p=0.95,  # Nucleus sampling (p > 0.95)\n",
    "    temperature=1.0,  # Controls creativity (1.0 = Neutral)\n",
    "    do_sample=True  # Enables sampling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The artificial intelligence system was developed in partnership with IBM and Intel in the first few years of this new project.\n",
      "\n",
      "The machines will also include three of the world's most popular smartphones – the Galaxy Nexus and the Note 9. The Android OS is\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Exploring Generation Parameters\n",
    "- Modify parameters like `max_length`, `temperature` (for creativity), or `top_k` (filtering the most probable words) to observe how they affect the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Model Hyperparameters\n",
    "\n",
    "##### 1. **`token_ids`**\n",
    "- **Description**: List of token IDs representing the input text.\n",
    "- **Role**: Allows the model to understand words as numerical data.\n",
    "- **Default Value**: Depends on the input text and the tokenizer used.\n",
    "- **Note**: Serves as the primary input for generating a response or text.\n",
    "\n",
    "##### 2. **`max_length`**\n",
    "- **Description**: Maximum length of the generated text.\n",
    "- **Role**: Limits the size of the generated sequence to avoid excessively long outputs.\n",
    "- **Default Value**: Variable depending on the model, often around 20-50.\n",
    "- **Example**: `max_length=100` will generate text with up to 100 tokens.\n",
    "\n",
    "##### 3. **`num_return_sequences`**\n",
    "- **Description**: Number of text sequences generated by the model for a single request.\n",
    "- **Role**: Useful for exploring multiple variations of the generated response.\n",
    "- **Default Value**: Usually 1.\n",
    "- **Example**: `num_return_sequences=3` generates 3 different responses for the same input.\n",
    "\n",
    "##### 4. **`no_repeat_ngram_size`**\n",
    "- **Description**: Minimum size of an n-gram that should not repeat.\n",
    "- **Role**: Prevents repetitions in the generated text.\n",
    "- **Default Value**: `None` or disabled.\n",
    "- **Example**: `no_repeat_ngram_size=2` prevents repetitions of two-word phrases (bigrams).\n",
    "\n",
    "##### 5. **`top_k`**\n",
    "- **Description**: Maximum number of tokens the model can choose from at each step of generation.\n",
    "- **Role**: Restricts selection to the `k` most probable options.\n",
    "- **Default Value**: 50 or `None` (no restriction).\n",
    "- **Example**: `top_k=10` limits consideration to the top 10 most probable options.\n",
    "\n",
    "##### 6. **`top_p`**\n",
    "- **Description**: Cumulative probability used for **Nucleus Sampling**.\n",
    "- **Role**: Limits selection to tokens whose cumulative probability is less than or equal to `top_p`.\n",
    "- **Default Value**: `1.0` (no limit).\n",
    "- **Example**: `top_p=0.95` considers only tokens with a cumulative probability ≤ 95%.\n",
    "\n",
    "##### 7. **`temperature`**\n",
    "- **Description**: Controls the diversity of the output by adjusting token probabilities.\n",
    "- **Role**: Lower temperature results in more conservative generation; higher temperature increases creativity.\n",
    "- **Default Value**: `1.0`.\n",
    "- **Example**:\n",
    "  - `temperature=0.1` → Predictable and conservative response.\n",
    "  - `temperature=1.5` → More varied and unpredictable response.\n",
    "\n",
    "##### 8. **`do_sample`**\n",
    "- **Description**: Enables or disables random sampling during generation.\n",
    "- **Role**: Determines whether the model chooses tokens based on probability (`do_sample=True`) or always selects the most probable token (`do_sample=False`).\n",
    "- **Default Value**: `False` (selects the most probable token).\n",
    "- **Example**:\n",
    "  - `do_sample=True` generates more varied responses.\n",
    "  - `do_sample=False` generates deterministic responses.\n",
    "\n",
    "#### **Practical Examples**\n",
    "##### Example 1: Precise and Structured Text\n",
    "```python\n",
    "max_length=50, top_k=5, temperature=0.3, do_sample=False\n",
    "```\n",
    "- Generates short text with limited options and minimal creativity.\n",
    "\n",
    "##### Example 2: Creative Text\n",
    "```python\n",
    "max_length=100, top_p=0.9, temperature=1.2, do_sample=True\n",
    "```\n",
    "- Generates longer, more varied text rich in lexical diversity.\n",
    "\n",
    "##### Example 3: Preventing Repetitions\n",
    "```python\n",
    "no_repeat_ngram_size=3, top_k=20, temperature=0.7\n",
    "```\n",
    "- Prevents 3-gram repetitions while balancing creativity and coherence.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate a text continuation\n",
    "output = model.generate(\n",
    "    token_ids,\n",
    "    max_length=100,  # Maximum length of the generated text\n",
    "    num_return_sequences=1,  # Number of sequences to generate\n",
    "    no_repeat_ngram_size=2,  # Prevent repetitions\n",
    "    top_k=10,  # Limit to a subset of probable words\n",
    "    top_p=0.95,  # Nucleus sampling (p > 0.95)\n",
    "    temperature=0.1,  # Controls creativity (1.0 is neutral)\n",
    "    do_sample=True  # Enables sampling\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The artificial intelligence system is able to predict the future, and it can also predict what will happen in the next few years.\n",
      "\n",
      "The system can then predict how long it will take to complete a task, how much time it takes to finish a sentence, what the expected outcome will be, etc. The system also can predict when the task will end. It can even predict whether the tasks will last for a certain amount of time. This is called the \"learning curve.\"\n",
      ". . .\n"
     ]
    }
   ],
   "source": [
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Analysis Questions\n",
    "- Compare the results based on the changed parameters. What happens when the temperature increases? What about when `top_k` is reduced?\n",
    "- What are the advantages and limitations of this type of model for text generation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis of Results Based on Parameters**\n",
    "\n",
    "#### **1. Effect of Temperature (`temperature`)**\n",
    "- Temperature controls the \"creativity\" or diversity in the model's choices for each generated word.\n",
    "- **Low Values (`<1.0`)**: \n",
    "  - The model becomes more conservative.\n",
    "  - Prioritizes the most probable words (deterministic decisions).\n",
    "  - Generates more coherent but less surprising text.\n",
    "  - **Example** (temperature = 0.7):\n",
    "    ```\n",
    "    The artificial intelligence system is designed to improve efficiency and solve complex problems in various industries.\n",
    "    ```\n",
    "- **High Values (`>1.0`)**:\n",
    "  - The model becomes more random.\n",
    "  - Increases the probability of unusual choices (less predictable decisions).\n",
    "  - Generates more creative text but may risk incoherence.\n",
    "  - **Example** (temperature = 1.5):\n",
    "    ```\n",
    "    The artificial intelligence system danced with paradoxical algorithms, weaving dreams in quantum fields of uncertainty.\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Effect of Reducing `top_k`**\n",
    "- `top_k` limits the number of possible choices at each step of generation.\n",
    "- **High Values (`>50`)**:\n",
    "  - The model has a broad range of words to choose from.\n",
    "  - Generates diverse but potentially less coherent text.\n",
    "  - **Example** (top_k = 100):\n",
    "    ```\n",
    "    The artificial intelligence system can adapt, predict, simulate, and conceptualize advanced scenarios to revolutionize industries.\n",
    "    ```\n",
    "- **Low Values (`<10`)**:\n",
    "  - Strongly restricts the model's choices.\n",
    "  - Generates more repetitive and predictable text.\n",
    "  - **Example** (top_k = 5):\n",
    "    ```\n",
    "    The artificial intelligence system is designed to improve performance and solve problems in various fields of technology.\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages and Limitations of This Model Type**\n",
    "\n",
    "#### **Advantages:**\n",
    "1. **Powerful Generalization**:\n",
    "   - Capable of generating convincing text across different domains and styles.\n",
    "2. **Customizable**:\n",
    "   - Parameters like `temperature`, `top_k`, and `top_p` allow adjustment of creativity and coherence.\n",
    "3. **Ease of Use**:\n",
    "   - Simple interfaces with libraries like `Transformers`.\n",
    "4. **Wide Applications**:\n",
    "   - Useful for writing, content creation, chatbots, and more.\n",
    "\n",
    "#### **Limitations:**\n",
    "1. **Contextual Inconsistencies**:\n",
    "   - Can generate grammatically correct but logically incorrect sentences.\n",
    "   - Example: \"The cat programmed the AI to solve equations.\"\n",
    "2. **Dependence on Training Data**:\n",
    "   - The model may reproduce biases present in the original data.\n",
    "3. **Lack of Deep Understanding**:\n",
    "   - The model doesn't truly \"understand\" concepts. It predicts the probabilities of the next words.\n",
    "4. **Control Challenges**:\n",
    "   - Hard to ensure relevance or factual accuracy of the generated content.\n",
    "5. **Computational Limitations**:\n",
    "   - Requires significant resources for large models (CPU/GPU, memory).\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "To generate effective text, parameters must be adjusted based on the use case:\n",
    "- **For coherent and formal text**: Use low `temperature` and high `top_k`.\n",
    "- **For creative or imaginative text**: Increase `temperature` and reduce `top_k`.\n",
    "\n",
    "However, these models cannot replace humans for tasks requiring deep understanding, such as critical analysis or fact-checking.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
