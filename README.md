# LLM

This repository contains four branches, each dedicated to a specific LLM lab:

## Branches and Labs

1. **LLM - TP1:**
    - Manipulate a pre-trained language model to understand how it generates text and explore the process of tokenization and text generation.
   
2. **LLM - TP2:**
    - Building on TP1 with further complexity regarding Manipulating a pre-trained language model to understand how it generates text and exploring the processes of tokenization and text generation.

3. **LLM - TP3:**
    - Dive deeper into text generation by experimenting with advanced parameters such as temperature and top_k, and analyze their effects on output creativity and coherence while using a pre-trained language model like GPT-2.

4. **LLM - TP4:**
    - Explore the self-attention mechanism, a core component of Transformer architectures. 
    - Learn to:
        1. Implement the self-attention mechanism by creating vector representations of words, calculating attention scores, and generating contextualized word vectors.
        2. Extend the implementation to multi-head attention, incorporating positional encoding to capture word order information.
        3. Visualize attention weights to analyze and understand relationships between words in a sentence.

5. **LLM - TP5:**
    - Learn the principles of pre-training and fine-tuning language models by first training a simple model, then fine-tuning BERT for binary text classification, such as identifying positive or negative reviews, while gaining hands-on experience in dataset adaptation, text processing, and performance evaluation.


