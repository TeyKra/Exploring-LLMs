{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP5: Pre-training and Fine-tuning a Language Model\n",
    "\n",
    "### Objectives\n",
    "1. Understand the basics of pre-training a simple model.  \n",
    "2. Fine-tune a pre-trained model (BERT) for a text classification task.  \n",
    "3. Work with real textual data and train a model on it.  \n",
    "\n",
    "### Part 1: Pre-training (Simple Example)  \n",
    "#### Introduction  \n",
    "Simulate simplified pre-training by training a small model to predict masked words in sentences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a Text Dataset  \n",
    "- Provide a simple corpus (or generate one) containing short sentences.  \n",
    "- Example corpus:  \n",
    "```\n",
    "The cat sleeps on the mat.  \n",
    "The dog plays in the garden.  \n",
    "The car is red.  \n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus has been created and saved in the file 'corpus.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Creating a simple corpus\n",
    "corpus = [\n",
    "    \"The cat sleeps on the mat.\",\n",
    "    \"The dog plays in the garden.\",\n",
    "    \"The car is red.\"\n",
    "]\n",
    "\n",
    "# Saving the corpus to a text file\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for sentence in corpus:\n",
    "        file.write(sentence + \"\\n\")\n",
    "\n",
    "print(\"The corpus has been created and saved in the file 'corpus.txt'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocess the Data  \n",
    "- **Tokenization**: Split sentences into words.  \n",
    "- **Word Masking**: Mask a random word in each sentence to predict it.  \n",
    "    - Example: \"The cat sleeps [MASK] the mat.\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: The cat sleeps on the mat.\n",
      "Tokenization: ['The', 'cat', 'sleeps', 'on', 'the', 'mat.']\n",
      "Masked sentence: The [MASK] sleeps on the mat. | Masked word: cat\n",
      "--------------------------------------------------\n",
      "Original sentence: The dog plays in the garden.\n",
      "Tokenization: ['The', 'dog', 'plays', 'in', 'the', 'garden.']\n",
      "Masked sentence: The dog plays in the [MASK] | Masked word: garden.\n",
      "--------------------------------------------------\n",
      "Original sentence: The car is red.\n",
      "Tokenization: ['The', 'car', 'is', 'red.']\n",
      "Masked sentence: The [MASK] is red. | Masked word: car\n",
      "--------------------------------------------------\n",
      "\n",
      "The preprocessed data has been saved to the file 'preprocessed_data.txt'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Loading the corpus\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.readlines()\n",
    "\n",
    "# Preprocessing: Tokenization and Masking\n",
    "preprocessed_data = []\n",
    "for sentence in corpus:\n",
    "    # Remove newline characters and tokenize\n",
    "    tokens = sentence.strip().split()\n",
    "    \n",
    "    # Display the tokenization\n",
    "    print(f\"Original sentence: {sentence.strip()}\")\n",
    "    print(f\"Tokenization: {tokens}\")\n",
    "    \n",
    "    # Choose a random word to mask\n",
    "    if len(tokens) > 1:  # Avoid masking an empty sentence or a single word\n",
    "        mask_index = random.randint(0, len(tokens) - 1)\n",
    "        original_word = tokens[mask_index]\n",
    "        tokens[mask_index] = \"[MASK]\"\n",
    "        \n",
    "        # Reconstruct the masked sentence\n",
    "        masked_sentence = \" \".join(tokens)\n",
    "        preprocessed_data.append((masked_sentence, original_word))  # Mask + masked word\n",
    "        \n",
    "        # Display the masked sentence\n",
    "        print(f\"Masked sentence: {masked_sentence} | Masked word: {original_word}\")\n",
    "        print(\"-\" * 50)  # Separator for readability\n",
    "\n",
    "# Saving the preprocessed data\n",
    "with open(\"preprocessed_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for masked, word in preprocessed_data:\n",
    "        file.write(f\"{masked} | {word}\\n\")\n",
    "\n",
    "print(\"\\nThe preprocessed data has been saved to the file 'preprocessed_data.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a Simple Model: Build a model capable of predicting a masked word in a sentence.  \n",
    "- **1. Add an embedding layer** (use `nn.Embedding`): Transform each word into a numerical vector.  \n",
    "- **2. Add a dense layer** (use `nn.Linear`): Identify complex relationships between words.  \n",
    "- **3. Add a softmax layer** (use `nn.Softmax`): Convert the outputs into probabilities to predict the masked word.  \n",
    "\n",
    "#### Minimal Model Structure:\n",
    "- **Input**: Indices of the words in the sentence.  \n",
    "- **Output**: Probabilities for each word in the vocabulary.  \n",
    "\n",
    "#### 4. Train the Model\n",
    "- **Task**: Predict masked words from their contexts.  \n",
    "- **Loss Function**: Cross-Entropy Loss.  \n",
    "- **dEmbedding** = 10  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.venv/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.venv/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.6819\n",
      "Masked sentence: The [MASK] sleeps on the mat.\n",
      "Predicted word: garden. (Original word: cat)\n",
      "Reconstructed sentence: The garden. sleeps on the mat.\n",
      "--------------------------------------------------\n",
      "Epoch 2/5, Loss: 2.6686\n",
      "Masked sentence: The [MASK] is red. <PAD> <PAD>\n",
      "Predicted word: car (Original word: car)\n",
      "Reconstructed sentence: The car is red. <PAD> <PAD>\n",
      "--------------------------------------------------\n",
      "Epoch 3/5, Loss: 2.6593\n",
      "Masked sentence: The dog plays in the [MASK]\n",
      "Predicted word: garden. (Original word: garden.)\n",
      "Reconstructed sentence: The dog plays in the garden.\n",
      "--------------------------------------------------\n",
      "Epoch 4/5, Loss: 2.6518\n",
      "Masked sentence: The [MASK] sleeps on the mat.\n",
      "Predicted word: cat (Original word: cat)\n",
      "Reconstructed sentence: The cat sleeps on the mat.\n",
      "--------------------------------------------------\n",
      "Epoch 5/5, Loss: 2.6300\n",
      "Masked sentence: The [MASK] sleeps on the mat.\n",
      "Predicted word: cat (Original word: cat)\n",
      "Reconstructed sentence: The cat sleeps on the mat.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Loading preprocessed data\n",
    "class MaskedDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = []\n",
    "        self.vocab = set()\n",
    "        \n",
    "        # Loading data\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                masked_sentence, word = line.strip().split(\" | \")\n",
    "                self.data.append((masked_sentence, word))\n",
    "                \n",
    "                # Building the vocabulary\n",
    "                self.vocab.update(masked_sentence.split())\n",
    "                self.vocab.add(word)\n",
    "        \n",
    "        # Adding the <PAD> token\n",
    "        self.vocab.add(\"<PAD>\")\n",
    "        \n",
    "        # Converting the vocabulary to indices\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        masked_sentence, word = self.data[idx]\n",
    "        sentence_indices = [self.word_to_idx[token] for token in masked_sentence.split()]\n",
    "        target_index = self.word_to_idx[word]\n",
    "        return torch.tensor(sentence_indices, dtype=torch.long), torch.tensor(target_index, dtype=torch.long)\n",
    "    \n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "# collate_fn function for padding\n",
    "def collate_fn(batch):\n",
    "    # Extract sentences and targets\n",
    "    sentences, targets = zip(*batch)\n",
    "    \n",
    "    # Find the maximum sentence length in the batch\n",
    "    max_len = max(len(sentence) for sentence in sentences)\n",
    "    \n",
    "    # Add padding to standardize lengths\n",
    "    padded_sentences = [torch.cat([sentence, torch.full((max_len - len(sentence),), dataset.word_to_idx[\"<PAD>\"], dtype=torch.long)]) for sentence in sentences]\n",
    "    \n",
    "    # Convert targets to a tensor\n",
    "    targets = torch.stack(targets)\n",
    "    \n",
    "    return torch.stack(padded_sentences), targets\n",
    "\n",
    "# Initializing the dataset and DataLoader\n",
    "file_path = \"preprocessed_data.txt\"\n",
    "dataset = MaskedDataset(file_path)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Creating the model\n",
    "class MaskedWordPredictionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(MaskedWordPredictionModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = self.embedding(x)\n",
    "        # Average embeddings for all words in the sentence\n",
    "        sentence_embedding = embeddings.mean(dim=1)\n",
    "        logits = self.fc(sentence_embedding)\n",
    "        probabilities = self.softmax(logits)\n",
    "        return probabilities\n",
    "\n",
    "# Parameters\n",
    "embedding_dim = 10\n",
    "vocab_size = dataset.vocab_size()\n",
    "model = MaskedWordPredictionModel(vocab_size, embedding_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sentences, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prediction\n",
    "        outputs = model(sentences)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # Test the model on an example after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentences, targets in dataloader:\n",
    "            example_sentence = sentences[0]\n",
    "            example_target = targets[0]\n",
    "            \n",
    "            # Prediction for an example\n",
    "            outputs = model(example_sentence.unsqueeze(0))  # Add a batch dimension\n",
    "            predicted_index = torch.argmax(outputs, dim=-1).item()\n",
    "            predicted_word = dataset.idx_to_word[predicted_index]\n",
    "            original_word = dataset.idx_to_word[example_target.item()]\n",
    "            \n",
    "            # Reconstruct the sentence with the predicted word\n",
    "            example_sentence_tokens = [dataset.idx_to_word[idx.item()] for idx in example_sentence]\n",
    "            reconstructed_sentence = \" \".join(\n",
    "                token if token != \"[MASK]\" else predicted_word for token in example_sentence_tokens\n",
    "            )\n",
    "            \n",
    "            print(f\"Masked sentence: {' '.join(example_sentence_tokens)}\")\n",
    "            print(f\"Predicted word: {predicted_word} (Original word: {original_word})\")\n",
    "            print(f\"Reconstructed sentence: {reconstructed_sentence}\")\n",
    "            print(\"-\" * 50)\n",
    "            break  # Display only the first example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
